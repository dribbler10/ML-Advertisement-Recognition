\section{Отбор объектов обучающей выборки}
Как правило, работа с выборками большого объёма сопряжена с большими затратами временина обучение модели. Кроме этого, в достаточно популярном методе \(k\) ближайших соседей обучающая выборка хранится полностью, что в случае выборки большого объёма является ограничиващим для него фактором. Таким образом, привлекательными кажутся техники, которые позволяют уменьшить объём обучающей выборки, почти не теряя обобщающей способности. Данная тематика в литературе на английском языке называется instance selection (prototype selection).

В \cite{ps-taxonomy} было проведено крупномасштабное исследование, а также классификация методов отбора объектов обучающей выборки. Авторы также классифицировали их на следующие группы по механизму работы:
\begin{itemize}
    \item методы сгущения (condensation) --- стремятся сократить число точек, далёких от границ классов, в предположении, что они слабо влияют на геометрию границы. Они стремятся сохранить качество классификации на обучающей выборке, при этом качество классификации на тестовой выборке может пострадать. Тем не менее, они, как правило, достигают высокой степени сокращения объёма обучающей выборки;
    \item методы редактирования (edition) --- стремятся сократить число точек, близких к границам классов, несогласованных с соседними, шумовых точек. Данные методы создают более гладкие границы между классами, и приводят к повышению обобщающей способности классификатора, но они в меньшей степени сокращают объём выборки, чем методы предыдущей группы;
    \item гибридные методы (hybrid) --- стремятся найти подмножество выборки как можно меньшей мощности, которое улучшает обобщающую способность на тестовых данных.
\end{itemize}

Столкнувшись со значительными затратами вычислительных ресурсов при тестировании различных методов машинного обучения в нашей задаче, мы решили опробовать несколько методов отбора объектов и оценить, насколько большую пользу они могут принести в данной задаче. В пакете scikit-learn данный класс методов не представлен, а существующие реализации на Python \cite{scikit-protopy} показали себя неудовлетворительно с точки зрения производительности. Поэтому для проведения данной работы некоторые методы, реализованные авторами \cite{ps-taxonomy}, были портированы на язык C. В следующих подразделах выбранные методы отбора объектов описаны и приведены результаты, показанные ими на нашей задаче.

\subsection{Condensed nearest neighbor}
Данный метод хронологически является одним из первых методов отбора эталонов. Он был описан в 1966 году в \cite{hart} вместе с понятием согласованного подмножества (consistent subset) обучающей выборки \(T\) --- такого подмножества \(S\subseteq T\), что метод ближайшего соседа, обученный на \(S\) правильно классифицирует \(T\setminus S\). Он представляет из себя простейший метод сгущения и оперирует с двумя множествами: \(S\) и \(R\). Изначально \(S\) содержит лишь первый элемент обучающей выборки, а \(R=\varnothing\). Второй элемент классифицируется методом ближайшего соседа на основе множества \(S\), и, если ему присвоен верный класс, то он добавляется в \(R\), иначе он добавляется в \(S\). Каждый следующий элемент \(T\) обрабатывается аналогичным образом. После первого прохода через \(T\) процедура начинает совершать проходы через множество \(R\), до тех пор, пока за весь проход через \(R\) ни один элемент не будёт перенесён в \(S\), после чего в \(S\) находится согласованное подмножество \(T\).

Метод CNN был сформулирован для метода одного ближайшего соседа, однако расширение на \(k\) ближайших соседей производится очевидным образом: при классификации очередного элемента \(T\) (а впоследствии \(R\)) используется метод \(k\)NN с обучающей выборкой \(S\). Стоит отметить, что данный метод строит согласованное подмножество только по отношению к методу \(k\) ближайших соседей (для конкретного \(k\)), результирующее подмножество \(S\) не обязано быть согласованным для произвольного алгоритма классификации.

В таблице~\ref{cnn-results-table} приведены результаты применения метода CNN:

\subsection{Fast condensed nearest neighbor}
Метод FCNN \cite{angiulli} был предложен с целью исправить некоторые недостатки CNN и других методов, основанных на \(k\)NN: зависимость результирующего подмножества \(S\) от порядка элементов в обучающей выборке и низкая производительность и масштабируемость. Сначала \(S\) содержит точки, ближайшие к барицентрам классов (барицентр класса --- \(\mathbf{x}_C=|C|^{-1}\sum_{c\in C}\mathbf{x}_c\), где \(C\) --- множество индексов обектов, принадлежащих одному классу). Затем, множество \(T\setminus S\) разбивается на \(|S|\) непересекающихся классов \(Vor(p, S, T)\) (так называмые Voronoi cells), в каждом из которых находятся объекты, для которых ближайшим соседом является один и тот же объект \(p\in S\). Затем все объекты из \(T\setminus S\) классифицируются методом ближайшего соседа с обучающей выборкой \(S\) и в каждом \(Vor(p, S, T)\) выбираются неверно классифицированные объекты, которые составляют множества \(Voren(p, S, T)\) (Voronoi enemies). После этого для каждого такого \(p\in S\), что \(Voren(p, S, T)\neq\varnothing\) выбирается ближайший к \(p\) объект из \(Voren(p, S, T)\) и они одновременно добавляются в \(S\). На каждой следующей итерации процедура повторяется для нового \(S\). Алгоритм останавливается, когда на последней итерации в \(S\) не было добавлено ни одного элемента.

FCNN может быть расширен на число ближайших соседей, отличных от 1, аналогично CNN: при классификации объектов в \(T\setminus S\) нужно использовать метод \(k\)NN. В \cite{angiulli} приведены доказательства того, что он строит согласованное подмножество \(S\), одинаковое для любого упорядочивания обучающей выборки.

В таблице~\ref{fcnn-results-table} приведены результаты применения метода FCNN:

\subsection{Class conditioned instance selection}
\subsection{Instance-based 3}
