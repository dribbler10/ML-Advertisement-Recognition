\section{Отбор эталонов}
Как правило, работа с выборками большого объёма сопряжена с большими затратами времени на обучение модели. Кроме этого, в достаточно популярном методе \(k\) ближайших соседей обучающая выборка хранится полностью, что в случае выборки большого объёма является ограничиващим для него фактором. Таким образом, привлекательными кажутся техники, которые позволяют уменьшить объём обучающей выборки, почти не теряя обобщающей способности. Данная тематика в литературе на английском языке называется instance selection (prototype selection).

В \cite{ps-taxonomy} было проведено крупномасштабное исследование, а также классификация методов отбора объектов обучающей выборки. Авторы разбили их на следующие группы по механизму работы:
\begin{itemize}
    \item методы сгущения (condensation) --- стремятся сократить число точек, далёких от границ классов, в предположении, что они слабо влияют на геометрию границы. Они стремятся сохранить качество классификации на обучающей выборке, при этом качество классификации на тестовой выборке может пострадать. Тем не менее, они, как правило, достигают высокой степени сокращения объёма обучающей выборки;
    \item методы редактирования (edition) --- стремятся сократить число точек, близких к границам классов, несогласованных с соседними, шумовых точек. Данные методы создают более гладкие границы между классами, и приводят к повышению обобщающей способности классификатора, но они в меньшей степени сокращают объём выборки, чем методы предыдущей группы;
    \item гибридные методы (hybrid) --- стремятся найти подмножество выборки как можно меньшей мощности, которое улучшает обобщающую способность на тестовых данных.
\end{itemize}

Столкнувшись со значительными затратами вычислительных ресурсов при тестировании различных методов машинного обучения в нашей задаче, мы решили опробовать несколько методов отбора эталонов и оценить, насколько большую пользу они могут принести в данной задаче. В пакете scikit-learn данный класс методов не представлен, а существующие реализации на Python \cite{scikit-protopy} показали себя неудовлетворительно с точки зрения производительности. Поэтому для проведения данной работы методы condensed nearest neighbor (CNN), fast condensed nearest neigbor (FCNN) и class conditional instance selection (CCIS), реализованные авторами \cite{ps-taxonomy}, были портированы на язык C. В следующих подразделах выбранные методы отбора эталонов описаны и приведены результаты, показанные ими на нашей задаче.

Все представленные в данном разделе методы используют \(k\)NN для внутренних нужд: оценки подмножеств обучающей выборки, нахождения новых элементов для добавления в неё или удаления из неё. Значительное количество методов, исследованных в \cite{ps-taxonomy} опираются на \(k\)NN (в частности, из-за того, что они чаще всего применяются в связке с \(k\)NN классификатором). Тем не менее, спектр методов отбора эталонов ими не ограничивается, часть методов в \cite{ps-taxonomy} относится к классу эволюционных. Некоторые из них могут агрессивно редуцировать обучающую выборку и обеспечивать хорошое качество классификации. К сожалению, данные методы требуют значительно больших затрат времени, чем рассматриваемые далее, что не позволило нам оценить их в рамках данной работы.

\subsection{Condensed nearest neighbor}
Данный метод хронологически является одним из первых методов отбора эталонов. Он был описан в 1966 году в \cite{hart} вместе с понятием согласованного подмножества (consistent subset) обучающей выборки \(T\) --- такого подмножества \(S\subseteq T\), что метод ближайшего соседа, обученный на \(S\), правильно классифицирует \(T\setminus S\). Он представляет из себя простейший метод сгущения и оперирует с двумя множествами: \(S\) и \(R\). Изначально \(S\) содержит лишь первый элемент обучающей выборки, а \(R=\varnothing\). Второй элемент классифицируется методом ближайшего соседа на основе множества \(S\), и, если ему присвоен верный класс, то он добавляется в \(R\), иначе он добавляется в \(S\). Каждый следующий элемент \(T\) обрабатывается аналогичным образом. После первого прохода через \(T\) процедура начинает совершать проходы через множество \(R\), до тех пор, пока за весь проход через \(R\) ни один элемент не будёт перенесён в \(S\), после чего в \(S\) находится согласованное подмножество \(T\).

Метод CNN был сформулирован для метода одного ближайшего соседа, однако расширение на \(k\) ближайших соседей производится очевидным образом: при классификации очередного элемента \(T\) (а впоследствии \(R\)) используется метод \(k\)NN с обучающей выборкой \(S\). Стоит отметить, что данный метод строит согласованное подмножество только по отношению к методу \(k\) ближайших соседей (для конкретного \(k\)), результирующее подмножество \(S\) не обязано быть согласованным для произвольного алгоритма классификации.

\subsubsection*{Результаты применения CNN}
Были проведены эксперименты с методом CNN для \(k\in\{1, 5, 10\}\). На рис.~\ref{fig:cnn-stats} указаны результаты, показанные непосредственно CNN: время работы и доля отобранных объектов. Оказалось, что в нашей задаче доля отобранных объектов не имеет ярко выраженной зависимости от \(k\). Время работы ожидаемо зависит от \(k\) линейно (в реализации был использован наивный алгоритм \(k\)NN с перебором всех соседей).

\begin{figure}[h!]
    \centering
	\begin{subfigure}{0.45\textwidth}
		\includegraphics[width=\textwidth]{images/cnn-stats.png}
		\caption{Доля отобранных объектов.}
	\end{subfigure}
	\begin{subfigure}{0.45\textwidth}
		\includegraphics[width=\textwidth]{images/cnn-TimeStats.png}
		\caption{Время, потраченное на отбор эталонов.}
	\end{subfigure}
	\caption{Результаты работы CNN.}\label{fig:cnn-stats}
\end{figure}

На рис.~\ref{fig:cnn-knn-results}--\ref{fig:cnn-gtb-results} приведены результаты работы базовых методов на обучающих выборках, редуцированных с помощью CNN. При использовании \(k\), отличных от 1, CNN выбирает примерно такое же количество эталонов, как и при \(k=1\), но сами эталоны оказываются хуже по качеству. Этот результат не выглядит неожиданным, если учесть, что CNN, будучи методом сгущения, стремится оставить объекты, близкие к границе классов --- объекты, у которых много соседей из другого класса. За исключением каналов TIMES NOW и CNN-IBN для метода SVM использование \(k>1\) также не приносит ускорения базового метода классификации, в основном, потому что \(\left|S\right|\) получается примерно таким же. Иными словами, в данной задаче нет смысла использовать CNN с \(k>1\).

Тот факт, что CNN строит согласованное подмножество для \(k\)NN, наводит на вопрос: насколько хороший результат покажет 15-NN на выходе CNN с \(k=15\)? Такой эксперимент был проведён с одним каналом NDTV и он привёл к неудовлетворительным результатам: качество классификации оказалось близким к случаю \(k=10\) (около 25\%).

\begin{figure}[h!]
    \centering
	\begin{subfigure}{0.45\textwidth}
		\includegraphics[width=\textwidth]{images/cnn-KNN.png}
		\caption{Качество классификации.}
	\end{subfigure}
	\begin{subfigure}{0.45\textwidth}
		\includegraphics[width=\textwidth]{images/cnn-KNNTime.png}
		\caption{Время обучения.}
	\end{subfigure}
	\caption{Результаты применения CNN для 15-NN.}\label{fig:cnn-knn-results}
\end{figure}

\begin{figure}[h!]
	\centering
	\begin{subfigure}{0.45\textwidth}
		\includegraphics[width=\textwidth]{images/cnn-LDA.png}
		\caption{Качество классификации.}
	\end{subfigure}
	\begin{subfigure}{0.45\textwidth}
		\includegraphics[width=\textwidth]{images/cnn-LDATime.png}
		\caption{Время обучения.}
	\end{subfigure}
	\caption{Результаты применения CNN для LDA.}\label{fig:cnn-lda-results}
\end{figure}

\begin{figure}[h!]
	\centering
	\begin{subfigure}{0.45\textwidth}
		\includegraphics[width=\textwidth]{images/cnn-SVM.png}
		\caption{Качество классификации.}
	\end{subfigure}
	\begin{subfigure}{0.45\textwidth}
		\includegraphics[width=\textwidth]{images/cnn-SVMTime.png}
		\caption{Время обучения.}
	\end{subfigure}
	\caption{Результаты применения CNN для SVM.}\label{fig:cnn-svm-results}
\end{figure}

\begin{figure}[h!]
	\centering
	\begin{subfigure}{0.45\textwidth}
		\includegraphics[width=\textwidth]{images/cnn-randforest.png}
		\caption{Качество классификации.}
	\end{subfigure}
	\begin{subfigure}{0.45\textwidth}
		\includegraphics[width=\textwidth]{images/cnn-randforestTime.png}
		\caption{Время обучения.}
	\end{subfigure}
	\caption{Результаты применения CNN для Random forest.}\label{fig:cnn-rf-results}
\end{figure}

\begin{figure}
	\centering
	\begin{subfigure}{0.45\textwidth}
		\includegraphics[width=\textwidth]{images/cnn-gradboosting.png}
		\caption{Качество классификации.}
	\end{subfigure}
	\begin{subfigure}{0.45\textwidth}
		\includegraphics[width=\textwidth]{images/cnn-gradboostingTime.png}
		\caption{Время обучения.}
	\end{subfigure}
	\caption{Результаты применения CNN для GTB}\label{fig:cnn-gtb-results}
\end{figure}

В таблице~\ref{table:cnn-results} приведены результаты применения метода CNN при \(k=1\), как наиболее эффективного варианта. \(R\) и \(T_{PS}\) обозначены процент объектов выборки, оставленный методом CNN, и время работы CNN, соответственно. Как и в таблице~\ref{table:base-all}, \(Q\) означает качество классификации (в скобках указано абсолютное значение, на которое \(Q\) изменилось по сравнению с соответствующим значением в таблице~\ref{table:base-all}), и \(T_{tr}\) означает время обучения (в скобках указано во сколько раз время обучения после редукции больше времени обучения на всей выборке).

Из таблицы~\ref{table:cnn-results} видно, что несмотря на то, что \(k=1\) --- наилучший параметр из опробованных, метод CNN сильно портит качество классификации базовых методов. Объём выборки сокращается в 2-3 раза, и это приводит к ощутимому ускорению <<тяжёлых>> базовых методов --- SVM (ускорение от 4 до 10 раз) и градиентного бустинга (ускорение примерно в 3 раза). На некоторых каналах потери качества классификации не выглядят катастрофическими (связки случайный лес+CNN-IBN, случайный лес+NDTV, градиентный бустинг+NDTV), а вместе с полученными в этих случаях ускорениями, можно считать, что данные пары получили пользу от отбора эталонов. Самым неудобным для CNN оказался канал BBC --- от него осталась почти половина объектов и качество классификации базовыми методами на нём пострадало больше всего.

\begin{table}[h!]
    \centering
    \begin{tabular}{|c||c||c|c|}
		\cline{2-4}
		\multicolumn{1}{c||}{} & Сжатие \(T\) & \(k\)NN & LDA \\
		\hline \hline
		\input{cnn-table1.txt}
	\end{tabular}
	\newline \vspace*{0.5cm} \newline
	\centering
	\begin{tabular}{|c||c|c|c|}
		\cline{2-4}
		\multicolumn{1}{c||}{} & SVM & Random forest & GTB \\
		\hline \hline
		\input{cnn-table2.txt}
    \end{tabular}
    \caption{Сводная таблица результатов метода CNN и базовых методов после применения CNN}
    \label{table:cnn-results}
\end{table}

\subsection{Fast condensed nearest neighbor}
Метод FCNN \cite{angiulli} был предложен с целью исправить некоторые недостатки CNN и других методов, основанных на \(k\)NN: зависимость результирующего подмножества \(S\) от порядка элементов в обучающей выборке и низкая производительность и масштабируемость. Сначала \(S\) содержит точки, ближайшие к барицентрам классов (барицентр класса --- \(\mathbf{x}_C=|C|^{-1}\sum_{c\in C}\mathbf{x}_c\), где \(C\) --- множество индексов обектов, принадлежащих одному классу). Затем, множество \(T\setminus S\) разбивается на \(|S|\) непересекающихся классов \(Vor(p, S, T)\) (так называмые Voronoi cells), в каждом из которых находятся объекты, для которых ближайшим соседом является один и тот же объект \(p\in S\). Затем все объекты из \(T\setminus S\) классифицируются методом ближайшего соседа с обучающей выборкой \(S\) и в каждом \(Vor(p, S, T)\) выбираются неверно классифицированные объекты, которые составляют множества \(Voren(p, S, T)\) (Voronoi enemies). После этого для каждого такого \(p\in S\), что \(Voren(p, S, T)\neq\varnothing\) выбирается ближайший к \(p\) объект из \(Voren(p, S, T)\) и они одновременно добавляются в \(S\). На каждой следующей итерации процедура повторяется для нового \(S\). Алгоритм останавливается, когда на последней итерации в \(S\) не было добавлено ни одного элемента.

FCNN может быть расширен на число ближайших соседей, отличных от 1, аналогично CNN: при классификации объектов в \(T\setminus S\) нужно использовать метод \(k\)NN. В \cite{angiulli} приведено доказательство того, что он строит согласованное подмножество \(S\), одинаковое для любого упорядочивания обучающей выборки.

Аналогично CNN, с FCNN были проведены эксперименты для \(k\in\{1,5,10\}\). Доли отобранных объектов и времена работы для каждого из каналов приведены на рис.~\ref{fig:fcnn-stats}.
\begin{figure}[h!]
    \centering
	\begin{subfigure}{0.45\textwidth}
		\includegraphics[width=\textwidth]{images/fcnn-stats.png}
		\caption{Доля отобранных объектов.}
	\end{subfigure}
	\begin{subfigure}{0.45\textwidth}
		\includegraphics[width=\textwidth]{images/fcnn-TimeStats.png}
		\caption{Время, потраченное на отбор эталонов.}
	\end{subfigure}
	\caption{Результаты работы FCNN.}\label{fig:fcnn-stats}
\end{figure}

На рис.~\ref{fig:fcnn-knn-results}--\ref{fig:fcnn-gtb-results} приведены результаты работы базовых методах на обучающих выборках, редуцированных с помощью FCNN. \tbd{analysis}

\begin{figure}[h!]
    \centering
	\begin{subfigure}{0.45\textwidth}
		\includegraphics[width=\textwidth]{images/fcnn-KNN.png}
		\caption{Качество классификации.}
	\end{subfigure}
	\begin{subfigure}{0.45\textwidth}
		\includegraphics[width=\textwidth]{images/fcnn-KNNTime.png}
		\caption{Время обучения.}
	\end{subfigure}
	\caption{Результаты применения FCNN для 15-NN.}\label{fig:fcnn-knn-results}
\end{figure}

\begin{figure}[h!]
	\centering
	\begin{subfigure}{0.45\textwidth}
		\includegraphics[width=\textwidth]{images/fcnn-LDA.png}
		\caption{Качество классификации.}
	\end{subfigure}
	\begin{subfigure}{0.45\textwidth}
		\includegraphics[width=\textwidth]{images/fcnn-LDATime.png}
		\caption{Время обучения.}
	\end{subfigure}
	\caption{Результаты применения FCNN для LDA.}\label{fig:fcnn-lda-results}
\end{figure}

\begin{figure}[h!]
	\centering
	\begin{subfigure}{0.45\textwidth}
		\includegraphics[width=\textwidth]{images/fcnn-SVM.png}
		\caption{Качество классификации.}
	\end{subfigure}
	\begin{subfigure}{0.45\textwidth}
		\includegraphics[width=\textwidth]{images/fcnn-SVMTime.png}
		\caption{Время обучения.}
	\end{subfigure}
	\caption{Результаты применения FCNN для SVM.}\label{fig:fcnn-svm-results}
\end{figure}

\begin{figure}[h!]
	\centering
	\begin{subfigure}{0.45\textwidth}
		\includegraphics[width=\textwidth]{images/fcnn-randforest.png}
		\caption{Качество классификации.}
	\end{subfigure}
	\begin{subfigure}{0.45\textwidth}
		\includegraphics[width=\textwidth]{images/fcnn-randforestTime.png}
		\caption{Время обучения.}
	\end{subfigure}
	\caption{Результаты применения FCNN для Random forest.}\label{fig:fcnn-rf-results}
\end{figure}

\begin{figure}
	\centering
	\begin{subfigure}{0.45\textwidth}
		\includegraphics[width=\textwidth]{images/fcnn-gradboosting.png}
		\caption{Качество классификации.}
	\end{subfigure}
	\begin{subfigure}{0.45\textwidth}
		\includegraphics[width=\textwidth]{images/fcnn-gradboostingTime.png}
		\caption{Время обучения.}
	\end{subfigure}
	\caption{Результаты применения FCNN для GTB}\label{fig:fcnn-gtb-results}
\end{figure}

В таблице~\ref{table:fcnn-results} приведены результаты применения метода FCNN с правилом одного ближайшего соседа. Использованы обозначения, аналогичные таблице~\ref{table:cnn-results}.
\begin{table}[h!]
    \centering
    \begin{tabular}{|c||c||c|c|}
    \cline{2-4}
    \multicolumn{1}{c||}{} & Сжатие \(T\) & \(k\)NN & LDA \\
    \hline \hline
	\input{fcnn-table1.txt}
\end{tabular}
\newline \vspace*{0.5cm} \newline
\begin{tabular}{|c||c|c|c|}
    \cline{2-4}
    \multicolumn{1}{c||}{} & SVM & Random forest & GTB \\
    \hline \hline
	\input{fcnn-table2.txt}
    \end{tabular}
    \caption{Сводная таблица результатов метода FCNN и базовых методов после применения FCNN}
    \label{table:fcnn-results}
\end{table}

Во-первых, стоит отметить, что FCNN показал худшие, чем CNN, результаты по редукции обучающей выборки и времени работы (тем не менее, согласно \cite{angiulli}, FCNN имеет лучшую асимптотику по сравнению с CNN и, возможно, соотношение по времени работы было бы иным при редукции сверхбольшого набора данных). Во-вторых, FCNN показал хорошие результаты по сохранению обобщающей способности классификатора: качество классификации упало не более, чем на 1.8\% для всех методов и каналов. По этому параметру FCNN обогнал CNN. В-третьих, FCNN также позволил значительно сократить время работы метода 13 ближайших соседей и случайного леса. LDA, как и в случае с CNN требовал значительно больше (в относительных числах, абсолютное время обучения у LDA оставалось очень малым) времени на обучение модели.

\subsection{Class conditional instance selection}
CCIS \cite{marchiori} отличается от двух предыдущих методов тем, что он принадлежит к категории гибридных методов отбора эталонов. Он состоит из двух этапов: классово-условный выбор эталонов (CC) и прорежение (THIN).

Центральным для данного метода является граф классово-условных ближайших соседей \(G=(V,E)\) --- орграф, вершинам \(v\in V\) которого соответствуют элементы \(T\), а ребро \((a,b)\in E\), если \(b\) есть ближайший сосед \(a\) для некоторого класса. \(G\) разбивается на 2 подграфа: \(G_{wc}=(V,E_{wc})\), в котором есть лишь те рёбра из \(G\), которые инцидентны двум вершинам одного класса и \(G_{bc}=(V,E_{bc})\), в котором есть лишь те рёбра из \(G\), которые инцидентны двум вершинам разных классов. Полустепень захода в первом называется внутриклассовой полустепенью захода (within class in-degree), во втором --- междуклассовой полустепенью захода (between class in-degree).

Классово-условный отбор эталонов составляет подмножество \(S\) обучающей выборки \(T\), добавляя в него элементы по одному пока Leave-one-out-ошибка, полученная методом ближайшего соседа на \(S\), убывает и больше LOO-ошибки, полученной на \(T\). Порядок, в котором элементы \(T\) выбираются для добавления в \(S\), обусловлен теоретико-информационным критерием, зависящим от полустепеней захода элемента в \(G_{wc}\) и \(G_{bc}\), и характеризующим то, насколько глубоко объект находится внутри своего класса. В частности, из \(T\) удалятся все объекты, для которых междуклассовая полустепень захода больше внутриклассовой (такие элементы, скорее всего, лежат близко к границе классов.

Прорежение призвано отфильтровать \(S\), оставив объекты, наиболее важные для определения границы классов. Для этого из \(S\) выбирается подмножество \(S_f\), состоящее из объектов с положительной междуклассовой полустепенью захода в графе \(G^S\) для выборки \(S\) (эти объекты считаются важными для определения границы между классами). Затем \(S_f\) дополняется объектами из \(S\setminus S_f\) пока LOO-ошибка на \(T\) при использовании \(S\) в качестве обучающей выборки уменьшается.

\begin{table}[h!]
    \centering
    \begin{tabular}{|c||c||c|c|}
    \cline{2-4}
    \multicolumn{1}{c||}{} & Сжатие \(T\) & \(k\)NN & LDA \\
    \hline \hline
	\input{ccis-table1.txt}
\end{tabular}
\newline \vspace*{0.5cm} \newline
\begin{tabular}{|c||c|c|c|}
    \cline{2-4}
    \multicolumn{1}{c||}{} & SVM & Random forest & GTB \\
    \hline \hline
	\input{ccis-table2.txt}
    \end{tabular}
    \caption{Сводная таблица результатов метода CCIS и базовых методов после применения CCIS}
    \label{table:ccis-results}
\end{table}

